{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import polars as pl\n\ntrain = pl.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/train.parquet')\nvalidA = pl.read_parquet('../input/otto-train-and-test-data-for-local-validation/test.parquet')\nvalidB = pl.read_parquet('../input/otto-train-and-test-data-for-local-validation/test_labels.parquet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-04T04:53:38.388407Z","iopub.execute_input":"2023-08-04T04:53:38.388743Z","iopub.status.idle":"2023-08-04T04:54:09.539891Z","shell.execute_reply.started":"2023-08-04T04:53:38.388717Z","shell.execute_reply":"2023-08-04T04:54:09.538857Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Get subset of data \nfraction_of_sessions = 0.2\n\ntrain_sessions = train['session'].sample(fraction=fraction_of_sessions, seed=42)\ntrain = train.filter(pl.col(\"session\").is_in(train_sessions))\ntrain = train.sort(\"session\")\n\nvalidation_sessions = validA['session'].sample(fraction=fraction_of_sessions, seed=42)\nvalidA = validA.filter(pl.col(\"session\").is_in(validation_sessions))\nvalidA = validA.sort(\"session\")\n\nvalidB = validB.filter(pl.col(\"session\").is_in(validation_sessions))\nvalidB = validB.sort(\"session\")\n\nprint(train.shape[0], validA.shape[0], validB.shape[0])\n\nprint(train, validA, validB)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:36:09.755698Z","iopub.execute_input":"2023-08-03T15:36:09.757074Z","iopub.status.idle":"2023-08-03T15:36:31.055680Z","shell.execute_reply.started":"2023-08-03T15:36:09.757005Z","shell.execute_reply":"2023-08-03T15:36:31.053324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filter for clicks events and train - does not take into account interaction between clicks","metadata":{}},{"cell_type":"code","source":"%%time\nEVENT_TYPE = 0\n\ntrain_df = train.filter(pl.col(\"type\") == EVENT_TYPE)\nsentences_df = train_df.groupby('session').agg(sentence = pl.col('aid'))\nsentences = sentences_df['sentence'].to_list()\n\nwith open(\"w2v_input.txt\", \"w\") as f:\n    for sentence in sentences:\n        f.write(\" \".join(map(str, sentence)) + \"\\n\")\n        \nfrom gensim.models import Word2Vec\n\nparams = {\n    'vector_size': 50,\n    'window': 3,\n    'epochs': 5,\n    'min_count': 1,\n    'sample': 1e-3,\n}\n\nw2v_model= Word2Vec(corpus_file=\"w2v_input.txt\", **params)\nw2v_model.save(\"word2vec_click.model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:37:03.172089Z","iopub.execute_input":"2023-08-03T15:37:03.172581Z","iopub.status.idle":"2023-08-03T16:11:14.965301Z","shell.execute_reply.started":"2023-08-03T15:37:03.172530Z","shell.execute_reply":"2023-08-03T16:11:14.963973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nEVENT_TYPE = 1\n\ntrain_df = train.filter(pl.col(\"type\") == EVENT_TYPE)\nsentences_df = train_df.groupby('session').agg(sentence = pl.col('aid'))\nsentences = sentences_df['sentence'].to_list()\n\nwith open(\"w2v_input.txt\", \"w\") as f:\n    for sentence in sentences:\n        f.write(\" \".join(map(str, sentence)) + \"\\n\")\n        \nfrom gensim.models import Word2Vec\n\nparams = {\n    'vector_size': 50,\n    'window': 3,\n    'epochs': 5,\n    'min_count': 1,\n    'sample': 1e-3,\n}\n\nw2v_model= Word2Vec(corpus_file=\"w2v_input.txt\", **params)\nw2v_model.save(\"word2vec_cart.model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T16:11:14.967657Z","iopub.execute_input":"2023-08-03T16:11:14.968774Z","iopub.status.idle":"2023-08-03T16:16:23.063521Z","shell.execute_reply.started":"2023-08-03T16:11:14.968735Z","shell.execute_reply":"2023-08-03T16:16:23.062238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nEVENT_TYPE = 2\n\ntrain_df = train.filter(pl.col(\"type\") == EVENT_TYPE)\nsentences_df = train_df.groupby('session').agg(sentence = pl.col('aid'))\nsentences = sentences_df['sentence'].to_list()\n\nwith open(\"w2v_input.txt\", \"w\") as f:\n    for sentence in sentences:\n        f.write(\" \".join(map(str, sentence)) + \"\\n\")\n        \nfrom gensim.models import Word2Vec\n\nparams = {\n    'vector_size': 50,\n    'window': 3,\n    'epochs': 5,\n    'min_count': 1,\n    'sample': 1e-3,\n}\n\nw2v_model= Word2Vec(corpus_file=\"w2v_input.txt\", **params)\nw2v_model.save(\"word2vec_order.model\")","metadata":{"execution":{"iopub.status.busy":"2023-08-03T16:16:23.065061Z","iopub.execute_input":"2023-08-03T16:16:23.065404Z","iopub.status.idle":"2023-08-03T16:18:04.946181Z","shell.execute_reply.started":"2023-08-03T16:16:23.065374Z","shell.execute_reply":"2023-08-03T16:18:04.944936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.similarities.annoy import AnnoyIndexer\nimport numpy as np\n\nannoy_index = AnnoyIndexer(w2v_model, 300)\n\ntype_weight = {0: 1, 1: 6, 2: 3}\ndef generate_candidates(df):\n    aids = df[\"aid\"].to_list()\n    types = df[\"type\"].to_list()\n    unique_aids = list(dict.fromkeys(aids[::-1]))\n\n    time_weights = np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n    aids_counter = {}\n    for aid, w, t in zip(aids, time_weights, types):\n        aids_counter[aid] = aids_counter.get(aid, 0) + 1 + w * type_weight[t]\n\n    aids_counter_sorted = sorted(aids_counter.items(), key=lambda x: x[1])\n    candidates = [k for k, v in aids_counter_sorted]\n\n    if len(candidates) <= 20:\n        secondary_candidates_counter = Counter()\n        for candidate in candidates:\n            secondary_candidates_counter.update(list(map(str, model.wv.most_similar(candidate.str(), topn=20, indexer=annoy_index))))  \n        secondary_candidates = [k for k, v in secondary_candidates_counter.most_common(20-len(candidates))]\n        return candidates[:20] + secondary_candidates[:-1]\n\n    return candidates[:20]","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:36:31.074360Z","iopub.status.idle":"2023-08-03T15:36:31.074813Z","shell.execute_reply.started":"2023-08-03T15:36:31.074607Z","shell.execute_reply":"2023-08-03T15:36:31.074633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validA = validA.filter(pl.col(\"type\") == EVENT_TYPE)\nalidA_df = validA.sort(['session', 'ts'], descending=[False, True]).groupby('session').agg(pl.col(\"aid\"))\nvalidA_df = validA_df.with_columns(pl.col(\"session\").apply(lambda x: generate_candidates(x)).alias('pred'))\n\nvalidB_df = validB.filter(pl.col(\"type\") == EVENT_TYPE)\n\ndef get_metric(gt, pred):\n    gt = set(gt.to_list())\n    pred = set(spred.to_list())\n    return gt.intersection(pred)\n    \npred_vs_gt = validA_df.join(validB_df, on=\"session\")\npred_vs_gt = pred_vs_gt.with_columns(pl.struct(pl.col(['ground_truth', 'pred'])).apply(lambda x: get_metric(x['ground_truth'], x['pred'])).alias(\"metric_nom\"),\n                                     pl.col(\"ground_truth\").list.lengths().alias('gt_len'))\npred_vs_gt = pred_vs_gt.with_columns(pl.when(pl.col(\"gt_len\") > 20).then(pl.col(\"gt_len\")).otherwise(20).alias(\"metric_denom\"))\n\nscore = sum(pred_vs_gt['metric_nom'])/sum(pred_vs_gt['metric_denom'])","metadata":{"execution":{"iopub.status.busy":"2023-08-03T15:36:31.077942Z","iopub.status.idle":"2023-08-03T15:36:31.079127Z","shell.execute_reply.started":"2023-08-03T15:36:31.078786Z","shell.execute_reply":"2023-08-03T15:36:31.078826Z"},"trusted":true},"execution_count":null,"outputs":[]}]}